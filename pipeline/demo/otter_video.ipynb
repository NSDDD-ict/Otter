{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otter Video Demo\n",
    "\n",
    "Current Otter Video is Otter-v0.2-DC (0612), means it’s trianed on MIMIC-IT-DC at June 12th. The code reads a video and uniformly extracts 16 frames, so avoid using excessively long videos if you want the model to generate specific descriptions.\n",
    "\n",
    "If your machine has over 16G GPU memory, you can run our model locally in fp16 mode for tasks like video labeling and identifying harmful content. For machines with over 36G GPU memory (by combining multiple cards with [device_map='auto'](https://huggingface.co/docs/accelerate/usage_guides/big_modeling) to one model different cards), you can run our model in the more accurate fp32 mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-25 16:29:11,803] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.9/dist-packages/cv2/../../lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lzo/lib: did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//workspace.byted.org')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/tiger/tez_deploy/conf')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/tiger/tez_deploy/tez')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/tiger/tez_deploy/conf'), PosixPath('/opt/tiger/tez_deploy/tez/*'), PosixPath('/opt/tiger/tez_deploy/tez/lib/*')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/data00/yarn/logs')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/data00/yarn/pid')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/10,172.16.0.0/12,169.254.0.0/16,192.168.0.0/16'), PosixPath('1,10.0.0.0/8,127.0.0.0/8,fd00'), PosixPath('/8,100.64.0.0/10,fe80'), PosixPath('byted.org,bytedance.net,.byted.org,.bytedance.net,localhost,.ecombdimg.com,.byteimg.com,127.0.0.1,')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8118'), PosixPath('http'), PosixPath('//sys-proxy-rd-relay.byted.org')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//luban-source.byted.org/repository/scm'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/mlx/workspace'), PosixPath('/opt/tiger/mlx_notebook_pysdk/mlx-pysdk'), PosixPath('/opt/tiger/lite_sdk')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('hub.byted.org/base/lab.pytorch2'), PosixPath('4f035aea6ac458a0f71d88e0976de585')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//reckon.bytedance.net'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ml.bytedance.net'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//lf6-config.bytetcc.com/obj/tcc-config-web')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import mimetypes\n",
    "import os\n",
    "from typing import Union\n",
    "import cv2\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from otter.modeling_otter import OtterForConditionalGeneration\n",
    "\n",
    "# Disable warnings\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current model version is configured for Otter-Image with max_num_frames set to None.\n",
      "Total Trainable param: 1.441012 B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b35cd441f894aad9e8b14a2152a2ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OtterForConditionalGeneration(\n",
       "  (lang_encoder): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32004, 4096, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (3): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (4-6): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (7): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (8-10): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (11): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (12-14): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (15): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (16-18): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (19): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (20-22): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (23): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (24-26): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (27): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (28-30): 3 x OtterLayer(\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (31): OtterLayer(\n",
       "          (gated_cross_attn_layer): OtterGatedCrossAttentionBlock(\n",
       "            (attn): OtterMaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (feed_forward): ModuleList(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32004, bias=False)\n",
       "  )\n",
       "  (vision_encoder): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(257, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (perceiver): OtterPerceiverResampler(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x OtterPerceiverBlock(\n",
       "        (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
       "        (feed_forward): ModuleList(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------- Main Function -------------------\n",
    "load_bit = \"fp32\"\n",
    "if load_bit == \"fp16\":\n",
    "    precision = {\"torch_dtype\": torch.float16}\n",
    "elif load_bit == \"bf16\":\n",
    "    precision = {\"torch_dtype\": torch.bfloat16}\n",
    "elif load_bit == \"fp32\":\n",
    "    precision = {\"torch_dtype\": torch.float32}\n",
    "\n",
    "# This model version is trained on MIMIC-IT DC dataset.\n",
    "# model = OtterForConditionalGeneration.from_pretrained(\"luodian/OTTER-9B-DenseCaption\", device_map=\"auto\", **precision)\n",
    "model = OtterForConditionalGeneration.from_pretrained(\"/mnt/bn/ecom-govern-maxiangqian-lq/lj/Otter/exp_result/final_hfckpt\", device_map=\"auto\", **precision)\n",
    "\n",
    "tensor_dtype = {\"fp16\": torch.float16, \"bf16\": torch.bfloat16, \"fp32\": torch.float32}[load_bit]\n",
    "\n",
    "model.text_tokenizer.padding_side = \"left\"\n",
    "tokenizer = model.text_tokenizer\n",
    "image_processor = transformers.CLIPImageProcessor()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Utility Functions -------------------\n",
    "\n",
    "\n",
    "def get_content_type(file_path):\n",
    "    content_type, _ = mimetypes.guess_type(file_path)\n",
    "    return content_type\n",
    "\n",
    "\n",
    "# ------------------- Image and Video Handling Functions -------------------\n",
    "\n",
    "\n",
    "def extract_frames(video_path, num_frames=32):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_step = total_frames // num_frames\n",
    "    frames = []\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, i * frame_step)\n",
    "        ret, frame = video.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame).convert(\"RGB\")\n",
    "            frames.append(frame)\n",
    "\n",
    "    video.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def get_image(url: str) -> Union[Image.Image, list]:\n",
    "    if \"://\" not in url:  # Local file\n",
    "        content_type = get_content_type(url)\n",
    "    else:  # Remote URL\n",
    "        content_type = requests.head(url, stream=True, verify=False).headers.get(\"Content-Type\")\n",
    "\n",
    "    if \"image\" in content_type:\n",
    "        if \"://\" not in url:  # Local file\n",
    "            return Image.open(url)\n",
    "        else:  # Remote URL\n",
    "            return Image.open(requests.get(url, stream=True, verify=False).raw)\n",
    "    elif \"video\" in content_type:\n",
    "        video_path = \"temp_video.mp4\"\n",
    "        if \"://\" not in url:  # Local file\n",
    "            video_path = url\n",
    "        else:  # Remote URL\n",
    "            with open(video_path, \"wb\") as f:\n",
    "                f.write(requests.get(url, stream=True, verify=False).content)\n",
    "        frames = extract_frames(video_path)\n",
    "        if \"://\" in url:  # Only remove the temporary video file if it was downloaded\n",
    "            os.remove(video_path)\n",
    "        return frames\n",
    "    else:\n",
    "        raise ValueError(\"Invalid content type. Expected image or video.\")\n",
    "\n",
    "\n",
    "# ------------------- OTTER Prompt and Response Functions -------------------\n",
    "\n",
    "\n",
    "def get_formatted_prompt(prompt: str) -> str:\n",
    "    return f\"<image>User: {prompt} GPT:<answer>\"\n",
    "\n",
    "\n",
    "def get_response(vision_x, prompt: str, model=None, image_processor=None, tensor_dtype=None, batch_size=2) -> str:\n",
    "    \n",
    "\n",
    "    lang_x = model.text_tokenizer(\n",
    "        [\n",
    "            get_formatted_prompt(prompt),\n",
    "        ],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Get the data type from model's parameters\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    # Convert tensors to the model's data type\n",
    "    # .unsqueeze(0).repeat(batch_size, 1, 1, 1, 1)\n",
    "    vision_x = vision_x.to(dtype=model_dtype)\n",
    "    lang_x_input_ids = lang_x[\"input_ids\"].repeat(batch_size, 1)\n",
    "    lang_x_attention_mask = lang_x[\"attention_mask\"].repeat(batch_size, 1)\n",
    "\n",
    "    bad_words_id = model.text_tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to(model.device),\n",
    "        lang_x=lang_x_input_ids.to(model.device),\n",
    "        attention_mask=lang_x_attention_mask.to(model.device),\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        bad_words_ids=bad_words_id,\n",
    "    )\n",
    "    parsed_output = (\n",
    "        model.text_tokenizer.decode(generated_text[0])\n",
    "        .split(\"<answer>\")[-1]\n",
    "        .lstrip()\n",
    "        .rstrip()\n",
    "        .split(\"<|endofchunk|>\")[0]\n",
    "        .lstrip()\n",
    "        .rstrip()\n",
    "        .lstrip('\"')\n",
    "        .rstrip('\"')\n",
    "    )\n",
    "    return parsed_output\n",
    "\n",
    "# video_url = \"/mnt/bn/ecom-govern-maxiangqian-lq/lj/data/dwq/test/test_creative/C_KT_6_0151_0235.mp4\"\n",
    "# prompts_input = \"Why is the video as a whole comedic?\"\n",
    "# frames_list = get_image(video_url)\n",
    "# print(f\"\\nPrompt: {prompts_input}\")\n",
    "# response = get_response(frames_list, prompts_input, model, image_processor, tensor_dtype)\n",
    "# print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/424 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/bn/ecom-govern-maxiangqian-lq/lj/data/dwq/test/test_humor/H_A_101_1433_1631.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a detailed account of the video's funny moment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_vars.py\", line 624, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'tensor' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a cup of water, a black and white kitten is swimming.\n",
      "Explain the comedic scene depicted in the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [06:33<15:18, 65.64s/it]\n",
      "  0%|          | 0/424 [06:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(prompts_input)\n\u001b[0;32m---> 38\u001b[0m     response \u001b[39m=\u001b[39m get_response(vision_x, prompts_input, model, image_processor, tensor_dtype, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m     39\u001b[0m     \u001b[39mprint\u001b[39m(response)\n\u001b[1;32m     40\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m response\n",
      "Cell \u001b[0;32mIn[53], line 83\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m(vision_x, prompt, model, image_processor, tensor_dtype, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m lang_x_attention_mask \u001b[39m=\u001b[39m lang_x[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mrepeat(batch_size, \u001b[39m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m bad_words_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtext_tokenizer([\u001b[39m\"\u001b[39m\u001b[39mUser:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGPT1:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGFT:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGPT:\u001b[39m\u001b[39m\"\u001b[39m], add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m---> 83\u001b[0m generated_text \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     84\u001b[0m     vision_x\u001b[39m=\u001b[39;49mvision_x\u001b[39m.\u001b[39;49mto(model\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m     85\u001b[0m     lang_x\u001b[39m=\u001b[39;49mlang_x_input_ids\u001b[39m.\u001b[39;49mto(model\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m     86\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mlang_x_attention_mask\u001b[39m.\u001b[39;49mto(model\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m     87\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     88\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     90\u001b[0m     bad_words_ids\u001b[39m=\u001b[39;49mbad_words_id,\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m parsed_output \u001b[39m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m     model\u001b[39m.\u001b[39mtext_tokenizer\u001b[39m.\u001b[39mdecode(generated_text[\u001b[39m0\u001b[39m])\n\u001b[1;32m     94\u001b[0m     \u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m<answer>\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m parsed_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/bn/ecom-govern-maxiangqian-lq/lj/Otter/otter/modeling_otter.py:1006\u001b[0m, in \u001b[0;36mOtterForConditionalGeneration.generate\u001b[0;34m(self, vision_x, lang_x, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     vision_x \u001b[39m=\u001b[39m vision_x\u001b[39m.\u001b[39mrepeat_interleave(num_beams, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_vision_x(vision_x\u001b[39m=\u001b[39mvision_x)\n\u001b[0;32m-> 1006\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlang_encoder\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39;49mlang_x,\n\u001b[1;32m   1008\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1009\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meoc_token_id,\n\u001b[1;32m   1010\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs,\n\u001b[1;32m   1011\u001b[0m )\n\u001b[1;32m   1013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39mclear_conditioned_layers()\n\u001b[1;32m   1014\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1310\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[39m# decoder-only models should use left-padding for generation\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n\u001b[0;32m-> 1310\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1311\u001b[0m         generation_config\u001b[39m.\u001b[39mpad_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m         \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39msum(inputs_tensor[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m generation_config\u001b[39m.\u001b[39mpad_token_id) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1313\u001b[0m     ):\n\u001b[1;32m   1314\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1315\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1317\u001b[0m         )\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1320\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_test_video_path(root_dir, name):\n",
    "    if name.startswith('H'):\n",
    "        path = os.path.join(root_dir, 'test_humor', name)\n",
    "    elif name.startswith('M'):\n",
    "        path = os.path.join(root_dir, 'test_magic', name)\n",
    "    else:\n",
    "        path = os.path.join(root_dir, 'test_creative', name)\n",
    "    return path\n",
    "\n",
    "# 读取/mnt/bn/ecom-govern-maxiangqian-lq/lj/data/dwq/annotation_with_ID/funqa_test_group_by_video.json\n",
    "with open('/mnt/bn/ecom-govern-maxiangqian-lq/lj/data/dwq/annotation_with_ID/funqa_test_group_by_video.json', 'r') as f:\n",
    "    datas = json.load(f)\n",
    "    \n",
    "for video_name, instructions in tqdm(datas.items(), total=len(datas)):\n",
    "    video_url = get_test_video_path('/mnt/bn/ecom-govern-maxiangqian-lq/lj/data/dwq/test', video_name)\n",
    "    print(video_url)\n",
    "    frames_list = get_image(video_url)\n",
    "    if isinstance(frames_list, Image.Image):\n",
    "        vision_x = image_processor.preprocess([frames_list], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n",
    "    elif isinstance(frames_list, list):  # list of video frames\n",
    "        vision_x = image_processor.preprocess(frames_list, return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input data. Expected PIL Image or list of video frames.\")\n",
    "    batch_size = 2\n",
    "    vision_x = vision_x.repeat(batch_size, 1, 1, 1, 1, 1)\n",
    "    for data in tqdm(instructions):\n",
    "        prompts_input = data['instruction']\n",
    "        task = data['task']\n",
    "        if task == 'H1' or task == 'C1' or task == 'M1':\n",
    "            data['predict'] = data['output']\n",
    "        else:\n",
    "            print(prompts_input)\n",
    "            \n",
    "            response = get_response(vision_x, prompts_input, model, image_processor, tensor_dtype, batch_size=batch_size)\n",
    "            print(response)\n",
    "            data['predict'] = response\n",
    "            with open('/mnt/bn/ecom-govern-maxiangqian-lq/lj/Otter/infer_data/test_res.jsonl', 'a+') as f:\n",
    "                f.write(json.dumps(data) + '\\n')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    video_url = input(\"Enter video path: \")  # Replace with the path to your video file, could be any common format.\n",
    "\n",
    "    frames_list = get_image(video_url)\n",
    "\n",
    "    while True:\n",
    "        prompts_input = input(\"Enter prompts: \")\n",
    "\n",
    "        if prompts_input.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        print(f\"\\nPrompt: {prompts_input}\")\n",
    "        response = get_response(frames_list, prompts_input, model, image_processor, tensor_dtype)\n",
    "        print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "new_lang_x_input_ids = torch.randn([1,19])\n",
    "# Assuming lang_x_input_ids is a PyTorch tensor\n",
    "batch_size = 2\n",
    "new_lang_x_input_ids = new_lang_x_input_ids.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 3, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
